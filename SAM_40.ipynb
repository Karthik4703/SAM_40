{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aQhxkj5fIyh",
        "outputId": "1fd78eca-ed0c-4634-9d09-7ca268ed812d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVqHLTbmfRI5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMMnxFy2fTL5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import pywt\n",
        "from scipy.stats import entropy\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import welch\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.metrics import structural_similarity as ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lQUJ5flfVbJ"
      },
      "outputs": [],
      "source": [
        "# Define the local folder path where you want to save the CSV files\n",
        "local_folder = '/content/local_data_folder'\n",
        "\n",
        "# Create the local folder if it doesn't exist\n",
        "os.makedirs(local_folder, exist_ok=True)\n",
        "\n",
        "# to clear the contents of the directory uncomment the next 2 lines\n",
        "# shutil.rmtree(local_folder, ignore_errors=True)\n",
        "# os.makedirs(local_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4R7mgNcf2Zj"
      },
      "outputs": [],
      "source": [
        "raw_data_folder = '/content/drive/MyDrive/Data/raw_data'\n",
        "# filtered_data_folder = '/content/drive/MyDrive/Data/filtered_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFAnOayDf-UD"
      },
      "outputs": [],
      "source": [
        "files = os.listdir(raw_data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "def calculate_alpha(eeg_data_signal):\n",
        "\n",
        "    # Define the alpha band frequency range\n",
        "    alpha_low = 8.0  # Lower alpha band frequency (Hz)\n",
        "    alpha_high = 12.0  # Upper alpha band frequency (Hz)\n",
        "\n",
        "    # Define the sampling frequency (replace with your actual sampling frequency)\n",
        "    sampling_frequency = 128  # For example, 1000 Hz\n",
        "\n",
        "    # Design a bandpass filter to extract the alpha band\n",
        "    def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "        nyquist = 0.5 * fs\n",
        "        low = lowcut / nyquist\n",
        "        high = highcut / nyquist\n",
        "        b, a = butter(order, [low, high], btype='band')\n",
        "        return b, a\n",
        "\n",
        "    def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        y = lfilter(b, a, data, axis=1)\n",
        "        return y\n",
        "\n",
        "    # Apply the alpha bandpass filter to the EEG data\n",
        "    alpha_band_data = butter_bandpass_filter(eeg_data_signal, alpha_low, alpha_high, sampling_frequency)\n",
        "    return alpha_band_data\n",
        "\n",
        "    # Now, alpha_band_data contains the filtered EEG data in the alpha frequency range.\n"
      ],
      "metadata": {
        "id": "mTR5_ftje01v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_beta(eeg_data_signal):\n",
        "    sampling_rate = 128\n",
        "    central_frequency = 22\n",
        "\n",
        "    sigma = 1 / (2 * np.pi * central_frequency)\n",
        "    scales = np.arange(13, 31)\n",
        "\n",
        "\n",
        "    morlet_coefficients = []\n",
        "    for channel in eeg_data:\n",
        "        coefficients, frequencies = pywt.cwt(channel, scales, 'morl', sampling_period=1/sampling_rate)\n",
        "        morlet_coefficients.append(coefficients)\n",
        "\n",
        "    beta_band_power_array = np.abs(np.array(morlet_coefficients)) ** 2 #This shape is 32,18,3200\n",
        "    collapsed_beta_band_power_array = np.mean(beta_band_power_array, axis = 1)\n",
        "\n",
        "    return collapsed_beta_band_power_array"
      ],
      "metadata": {
        "id": "uOKcbWiDSOlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohJtSnDtTSgK"
      },
      "outputs": [],
      "source": [
        "# loopnumber=0\n",
        "test_names = [\"Mirror_image_sub\", \"Stroop_sub\", \"Arithmetic_sub\", \"Relax_sub\"]\n",
        "\n",
        "\n",
        "beta_shape = (480, 32, 3200)\n",
        "beta_aggregate = np.zeros(beta_shape)\n",
        "alpha_aggregate = np.zeros(beta_shape)\n",
        "\n",
        "# Loop through each .mat file in the folder\n",
        "loopnumber = 0\n",
        "\n",
        "for test_name in test_names:\n",
        "    for file_name in files:\n",
        "        if file_name.endswith('.mat')and file_name.startswith(test_name):\n",
        "            data = loadmat(os.path.join(raw_data_folder, file_name))\n",
        "\n",
        "            x = data['Data']\n",
        "            df = pd.DataFrame(x)\n",
        "        # Save the DataFrame as a CSV file in the local folder\n",
        "            csv_file_name = os.path.splitext(file_name)[0] + '.csv'\n",
        "            csv_file_path = os.path.join(local_folder, csv_file_name)\n",
        "            df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "            eeg_data = pd.read_csv(csv_file_path)\n",
        "            eeg_array = eeg_data.values\n",
        "        # print(eeg_array.size)\n",
        "            eeg_array = eeg_array.T\n",
        "            n_components = 4\n",
        "            independent_components = np.zeros((n_components, eeg_array.shape[0], eeg_array.shape[1]))\n",
        "\n",
        "            ica = FastICA(n_components=n_components, random_state=0)\n",
        "            for i in range(eeg_array.shape[0]):\n",
        "                channel_data = eeg_array[i, :].reshape(-1, 1)\n",
        "                independent_channel = ica.fit_transform(channel_data)\n",
        "                independent_components[:, i, :] = independent_channel.T\n",
        "\n",
        "\n",
        "            ica_data = independent_components[0, :, :]\n",
        "            ica_data=ica_data.T # ica_data.shape\n",
        "\n",
        "\n",
        "            rows_indices = [i for i in range(32)]\n",
        "\n",
        "            stress = ica_data[rows_indices, :]\n",
        "\n",
        "            eeg_data = stress\n",
        "\n",
        "            alpha_array = calculate_alpha(eeg_data)\n",
        "            collapsed_beta_band_power_array = calculate_beta(eeg_data)\n",
        "\n",
        "            beta_aggregate[loopnumber] = collapsed_beta_band_power_array\n",
        "            alpha_aggregate[loopnumber] = alpha_array\n",
        "            loopnumber = loopnumber + 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/My Drive/32_chnl/raw_data_Beta.npy', beta_aggregate)\n",
        "np.save('/content/drive/My Drive/32_chnl/raw_data_Alpha.npy', alpha_aggregate)"
      ],
      "metadata": {
        "id": "sFW4vH2XDFS2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "941ca7a3-21c6-4d0a-b7fd-cfa85134915d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d8dfc77ac5a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/32_chnl/raw_data_Beta.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_aggregate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/32_chnl/raw_data_Alpha.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_aggregate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'beta_aggregate' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta_aggregate = np.load('/content/drive/My Drive/32_chnl/raw_data_Beta.npy')\n",
        "alpha_aggregate = np.load('/content/drive/My Drive/32_chnl/raw_data_Alpha.npy')"
      ],
      "metadata": {
        "id": "R9Pl0dP-DMxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rweFEFbZ_LAk"
      },
      "outputs": [],
      "source": [
        "# Split the original array into four smaller arrays\n",
        "taskwise_beta_split = np.split(beta_aggregate, 4, axis=0)\n",
        "\n",
        "# Each split_array will have shape (120, 32, 3200)\n",
        "beta_Mirror_task, beta_stroop_task, beta_arthimetic_task, beta_rest_task = taskwise_beta_split\n",
        "# print(beta_Mirror_task.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the original array into four smaller arrays\n",
        "taskwise_alpha_split = np.split(alpha_aggregate, 4, axis=0)\n",
        "\n",
        "# Each split_array will have shape (120, 32, 3200)\n",
        "alpha_Mirror_task, alpha_stroop_task, alpha_arthimetic_task, alpha_rest_task = taskwise_alpha_split\n",
        "# print(beta_Mirror_task.shape)"
      ],
      "metadata": {
        "id": "D2vSX4XsomGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(input_array):\n",
        "    entropy_values = np.zeros((120, 1))\n",
        "    i = 0\n",
        "    for data in input_array:\n",
        "        data_T = data.reshape(3200,)\n",
        "\n",
        "        num_bins = int(np.sqrt(len(data_T)))  # Adjust the number of bins as needed\n",
        "        # num_bins = 20  # Adjust the number of bins as needed\n",
        "\n",
        "        # Calculate the histogram and probabilities\n",
        "        hist, bin_edges = np.histogram(data_T, bins=num_bins, density=True)\n",
        "        probabilities = hist / np.sum(hist)\n",
        "\n",
        "        # Calculate entropy using scipy's entropy function\n",
        "        data_entropy = entropy(probabilities, base=2)\n",
        "        # print(\"data entropy is\",data_entropy,\"\\n\")\n",
        "        entropy_values[i] = data_entropy\n",
        "        i += 1\n",
        "    return entropy_values"
      ],
      "metadata": {
        "id": "wJi1U05vGnZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG1vHAxUzvmV"
      },
      "outputs": [],
      "source": [
        "def calculate_psd(input_array, sampling_rate):\n",
        "    # Calculate PSD for a 3D array of shape (120, 1, 3200) and returns a 2D array of shape (120, 1) with PSD values.\n",
        "    # Reshape the array to (120, 3200) by removing the middle dimension\n",
        "    reshaped_array = input_array[:, 0, :]\n",
        "\n",
        "    # Initialize an array to store the PSD values\n",
        "    psd_array = np.zeros((120, 1))\n",
        "\n",
        "    # Calculate PSD for each signal in the reshaped array\n",
        "    for i in range(120):\n",
        "        signal = reshaped_array[i, :]\n",
        "        f, pxx = welch(signal, fs=sampling_rate, nperseg=256)  # Adjust nperseg as needed\n",
        "        psd_value = np.mean(pxx)  # You can use np.sum(pxx) for total power instead of mean\n",
        "        psd_array[i, 0] = psd_value\n",
        "\n",
        "    return psd_array\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_matrix_1 = np.empty((0, 96))\n",
        "\n",
        "for task in taskwise_beta_split:\n",
        "    each_task_beta_dimensions = (120, 32, 3200)\n",
        "    channels = []\n",
        "\n",
        "    for i in range(each_task_beta_dimensions[1]):\n",
        "        reshaped_array = task[:, i:i + 1, :]\n",
        "        channels.append(reshaped_array)\n",
        "\n",
        "\n",
        "    featurerow = np.mean(channels[0], axis=2)\n",
        "\n",
        "    for i in range(31):\n",
        "        channel = channels[i+1]\n",
        "        mean_array = np.mean(channel, axis=2)\n",
        "        featurerow = np.hstack((featurerow, mean_array))\n",
        "\n",
        "    for i in range(32):\n",
        "        channel = channels[i]\n",
        "        # Calculate entropy and add to the feature row\n",
        "        entropy_array = calculate_entropy(channel)\n",
        "        featurerow = np.hstack((featurerow, entropy_array))\n",
        "\n",
        "    for i in range(32):\n",
        "        channel = channels[i]\n",
        "        # Calculate PSD and add to the feature row\n",
        "        psd_array = calculate_psd(channel, 128)\n",
        "        featurerow = np.hstack((featurerow, psd_array))\n",
        "\n",
        "    # Append the feature row to the feature matrix\n",
        "    feature_matrix_1 = np.vstack((feature_matrix_1, featurerow))\n",
        "\n",
        "# Ensure feature_matrix dimensions are correct (number of rows, 96 columns)\n",
        "print(feature_matrix_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoRk4CRT1A68",
        "outputId": "1cbb8742-e93d-4dd0-eea3-2b95d111b2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix_2 = np.empty((0, 96))\n",
        "for task in taskwise_alpha_split:\n",
        "    each_task_alpha_dimensions = (120, 32, 3200)\n",
        "    channels = []\n",
        "\n",
        "    for i in range(each_task_alpha_dimensions[1]):\n",
        "        reshaped_array = task[:, i:i + 1, :]\n",
        "        channels.append(reshaped_array)\n",
        "\n",
        "\n",
        "    featurerow = np.mean(channels[0], axis=2)\n",
        "\n",
        "    for i in range(31):\n",
        "        channel = channels[i+1]\n",
        "        mean_array = np.mean(channel, axis=2)\n",
        "        featurerow = np.hstack((featurerow, mean_array))\n",
        "\n",
        "    for i in range(32):\n",
        "        channel = channels[i]\n",
        "        # Calculate entropy and add to the feature row\n",
        "        entropy_array = calculate_entropy(channel)\n",
        "        featurerow = np.hstack((featurerow, entropy_array))\n",
        "\n",
        "    # print(featurerow.min())\n",
        "\n",
        "    for i in range(32):\n",
        "        channel = channels[i]\n",
        "        # Calculate PSD and add to the feature row\n",
        "        psd_array = calculate_psd(channel, 128)\n",
        "        featurerow = np.hstack((featurerow, psd_array))\n",
        "\n",
        "    # Append the feature row to the feature matrix\n",
        "    feature_matrix_2 = np.vstack((feature_matrix_2, featurerow))\n",
        "\n",
        "# Ensure feature_matrix dimensions are correct (number of rows, 96 columns)\n",
        "print(feature_matrix_2.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGrAxbxOpjee",
        "outputId": "ae07dc25-acf5-4494-df8e-e89ddb2c596c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix = np.hstack((feature_matrix_1, feature_matrix_2))"
      ],
      "metadata": {
        "id": "CobPlRtFqcSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYhqeMSfqnQ_",
        "outputId": "51769cb2-55d2-4ece-c01e-fbffd2f79f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(480, 192)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'feature_matrix' is your NumPy array\n",
        "df = pd.DataFrame(feature_matrix)\n",
        "df.to_csv('/content/drive/My Drive/32_chnl/raw_feature_matrix.csv', index=False)"
      ],
      "metadata": {
        "id": "y5ciCsCoriKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjLFJnuoh9gi"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv('/content/drive/My Drive/32_chnl/raw_feature_matrix.csv')\n",
        "y = np.concatenate([np.ones(360), np.zeros(120)])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEYh9fBUl8Sq"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Assuming X is your feature matrix and y is your target labels\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "logistic_regression_model = LogisticRegression()\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "logistic_regression_predictions = logistic_regression_model.predict(X_test)\n",
        "logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_predictions)\n",
        "logistic_regression_f1 = f1_score(y_test, logistic_regression_predictions, average='weighted')\n",
        "logistic_regression_auc_roc = roc_auc_score(y_test, logistic_regression_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "svm_model = SVC(kernel='linear')  # You can choose different kernel functions\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "svm_f1 = f1_score(y_test, svm_predictions, average='weighted')\n",
        "svm_auc_roc = roc_auc_score(y_test, svm_model.decision_function(X_test))\n",
        "\n",
        "# Random Forest\n",
        "random_forest_model = RandomForestClassifier()\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "random_forest_predictions = random_forest_model.predict(X_test)\n",
        "random_forest_accuracy = accuracy_score(y_test, random_forest_predictions)\n",
        "random_forest_f1 = f1_score(y_test, random_forest_predictions, average='weighted')\n",
        "random_forest_auc_roc = roc_auc_score(y_test, random_forest_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# Gradient Boosting\n",
        "gradient_boosting_model = GradientBoostingClassifier()\n",
        "gradient_boosting_model.fit(X_train, y_train)\n",
        "gradient_boosting_predictions = gradient_boosting_model.predict(X_test)\n",
        "gradient_boosting_accuracy = accuracy_score(y_test, gradient_boosting_predictions)\n",
        "gradient_boosting_f1 = f1_score(y_test, gradient_boosting_predictions, average='weighted')\n",
        "gradient_boosting_auc_roc = roc_auc_score(y_test, gradient_boosting_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# K-Nearest Neighbors (K-NN)\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "knn_model.fit(X_train, y_train)\n",
        "knn_predictions = knn_model.predict(X_test)\n",
        "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
        "knn_f1 = f1_score(y_test, knn_predictions, average='weighted')\n",
        "knn_auc_roc = roc_auc_score(y_test, knn_model.predict_proba(X_test)[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Logistic Regression Accuracy: {logistic_regression_accuracy * 100:.2f}%\")\n",
        "print(f\"SVM Accuracy: {svm_accuracy * 100:.2f}%\")\n",
        "print(f\"Random Forest Accuracy: {random_forest_accuracy * 100:.2f}%\")\n",
        "print(f\"Gradient Boosting Accuracy: {gradient_boosting_accuracy * 100:.2f}%\")\n",
        "print(f\"K-NN Accuracy: {knn_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmm-ZRFogfLs",
        "outputId": "4a16b505-92d7-457d-cb90-2447ac47e123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 69.79%\n",
            "SVM Accuracy: 68.75%\n",
            "Random Forest Accuracy: 72.92%\n",
            "Gradient Boosting Accuracy: 70.83%\n",
            "K-NN Accuracy: 71.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Logistic Regression F1 Score: {logistic_regression_f1 * 100:.2f}%\")\n",
        "print(f\"SVM F1 Score: {svm_f1 * 100:.2f}%\")\n",
        "print(f\"Random Forest F1 Score: {random_forest_f1 * 100:.2f}%\")\n",
        "print(f\"Gradient Boosting F1 Score: {gradient_boosting_f1 * 100:.2f}%\")\n",
        "print(f\"knn F1 Score: {knn_f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfC3j4gRfgsE",
        "outputId": "0858fa80-4163-4f0c-8774-759020149cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression F1 Score: 64.09%\n",
            "SVM F1 Score: 65.30%\n",
            "Random Forest F1 Score: 63.29%\n",
            "Gradient Boosting F1 Score: 63.54%\n",
            "knn F1 Score: 69.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Logistic Regression AUC-ROC Score: {logistic_regression_auc_roc * 100:.2f}%\")\n",
        "print(f\"SVM AUC-ROC Score: {svm_auc_roc * 100:.2f}%\")\n",
        "print(f\"Random Forest AUC-ROC Score: {random_forest_auc_roc * 100:.2f}%\")\n",
        "print(f\"Gradient Boosting AUC-ROC Score: {gradient_boosting_auc_roc * 100:.2f}%\")\n",
        "print(f\"K-NN AUC-ROC Score: {knn_auc_roc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxCsPEB1iGX_",
        "outputId": "1ea57d62-bb9e-4c23-f1ed-75679d157546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression AUC-ROC Score: 67.91%\n",
            "SVM AUC-ROC Score: 62.80%\n",
            "Random Forest AUC-ROC Score: 67.53%\n",
            "Gradient Boosting AUC-ROC Score: 68.52%\n",
            "K-NN AUC-ROC Score: 74.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "channels = [\"CZ\", \"FZ\", \"Fp1\", \"F7\", \"F3\", \"FC1\", \"C3\", \"FC5\", \"FT9\", \"T7\", \"CP5\", \"CP1\", \"P3\", \"P7\", \"PO9\", \"O1\", \"PZ\", \"OZ\", \"O2\", \"PO10\", \"P8\", \"P4\", \"CP2\", \"CP6\", \"T8\", \"FT10\", \"FC6\", \"C4\", \"FC2\", \"F4\", \"F8\", \"Fp2\"]\n",
        "\n",
        "# Create a mapping dictionary from numbers to channel names\n",
        "channel_mapping = {i+1: channel for i, channel in enumerate(channels)}\n",
        "\n",
        "# Check the mapping\n",
        "print(channel_mapping)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZbz8DDy6Law",
        "outputId": "c0558493-c126-49b9-b724-f760aed2677a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'CZ', 2: 'FZ', 3: 'Fp1', 4: 'F7', 5: 'F3', 6: 'FC1', 7: 'C3', 8: 'FC5', 9: 'FT9', 10: 'T7', 11: 'CP5', 12: 'CP1', 13: 'P3', 14: 'P7', 15: 'PO9', 16: 'O1', 17: 'PZ', 18: 'OZ', 19: 'O2', 20: 'PO10', 21: 'P8', 22: 'P4', 23: 'CP2', 24: 'CP6', 25: 'T8', 26: 'FT10', 27: 'FC6', 28: 'C4', 29: 'FC2', 30: 'F4', 31: 'F8', 32: 'Fp2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming you have a feature matrix X of shape (480, 96)\n",
        "# where each row represents a sample (EEG recording) and each column is a feature\n",
        "\n",
        "# Define your target variable (stress levels), let's call it y\n",
        "# Make sure y is of shape (480,)\n",
        "y = np.concatenate([np.ones(360), np.zeros(120)])\n",
        "\n",
        "# Scale your feature matrix\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Calculate mutual information scores for each channel\n",
        "mi_scores = mutual_info_classif(X_scaled, y)\n",
        "\n",
        "# Create a list of (channel index, channel name, mutual information score) tuples\n",
        "channel_scores = [(i, channel_mapping[i], score) for i, score in enumerate(mi_scores) if i in channel_mapping]\n",
        "\n",
        "# Sort the list by mutual information score in descending order\n",
        "channel_scores.sort(key=lambda x: x[2], reverse=True)\n",
        "z=1\n",
        "# Print the channels with their mutual information scores in decreasing order\n",
        "for i, channel, score in channel_scores:\n",
        "    print(f\"{z}\\t Channel {i} - {channel}: Mutual Information Score = {score}\")\n",
        "    z += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHzry8tkP2iI",
        "outputId": "91b0c36b-465e-4d61-abf7-7145cd42a4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t Channel 27 - FC6: Mutual Information Score = 0.0566038026139597\n",
            "2\t Channel 30 - F4: Mutual Information Score = 0.0466492246072423\n",
            "3\t Channel 10 - T7: Mutual Information Score = 0.03740851384866817\n",
            "4\t Channel 29 - FC2: Mutual Information Score = 0.035322690433829296\n",
            "5\t Channel 32 - Fp2: Mutual Information Score = 0.030122597233637416\n",
            "6\t Channel 14 - P7: Mutual Information Score = 0.027592471474403357\n",
            "7\t Channel 17 - PZ: Mutual Information Score = 0.02707825969120048\n",
            "8\t Channel 15 - PO9: Mutual Information Score = 0.025442390378667135\n",
            "9\t Channel 12 - CP1: Mutual Information Score = 0.02527606481843958\n",
            "10\t Channel 25 - T8: Mutual Information Score = 0.02354290814387472\n",
            "11\t Channel 16 - O1: Mutual Information Score = 0.020351003724386807\n",
            "12\t Channel 19 - O2: Mutual Information Score = 0.020090487507449906\n",
            "13\t Channel 26 - FT10: Mutual Information Score = 0.017653252573319733\n",
            "14\t Channel 20 - PO10: Mutual Information Score = 0.01684836954059521\n",
            "15\t Channel 4 - F7: Mutual Information Score = 0.016263948221025837\n",
            "16\t Channel 3 - Fp1: Mutual Information Score = 0.011754595224120479\n",
            "17\t Channel 1 - CZ: Mutual Information Score = 0.011569733229833234\n",
            "18\t Channel 5 - F3: Mutual Information Score = 0.009410422990851375\n",
            "19\t Channel 8 - FC5: Mutual Information Score = 0.00856700074372152\n",
            "20\t Channel 21 - P8: Mutual Information Score = 0.004881027361824275\n",
            "21\t Channel 13 - P3: Mutual Information Score = 0.004120956853472801\n",
            "22\t Channel 18 - OZ: Mutual Information Score = 0.002862115693557099\n",
            "23\t Channel 7 - C3: Mutual Information Score = 0.001196383593226047\n",
            "24\t Channel 2 - FZ: Mutual Information Score = 0.0\n",
            "25\t Channel 6 - FC1: Mutual Information Score = 0.0\n",
            "26\t Channel 9 - FT9: Mutual Information Score = 0.0\n",
            "27\t Channel 11 - CP5: Mutual Information Score = 0.0\n",
            "28\t Channel 22 - P4: Mutual Information Score = 0.0\n",
            "29\t Channel 23 - CP2: Mutual Information Score = 0.0\n",
            "30\t Channel 24 - CP6: Mutual Information Score = 0.0\n",
            "31\t Channel 28 - C4: Mutual Information Score = 0.0\n",
            "32\t Channel 31 - F8: Mutual Information Score = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "channels = [27, 30]\n",
        "for channel in channels:\n",
        "  selected_columns = [channel+63]\n",
        "  psd = [0, 0, 0, 0]\n",
        "  for i in range(4):\n",
        "    selected_rows = [(i*120) + a for a in range(120)]\n",
        "    psd[i] = np.mean(feature_matrix[selected_rows][:, selected_columns])\n",
        "\n",
        "  print(\"for channel \", channel_mapping[channel])\n",
        "  print(\"\\tMirror Image Test : \", psd[0])\n",
        "  print(\"\\tStroop Color Test : \", psd[1])\n",
        "  print(\"\\tArithmetic Test   : \", psd[2])\n",
        "  print(\"\\tRelax state       : \", psd[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMqa6INSbAAr",
        "outputId": "67884022-696d-402f-8f48-27c5c4293045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for channel  FC6\n",
            "\tMirror Image Test :  1.9391661335766255e-05\n",
            "\tStroop Color Test :  1.5457594697470777e-05\n",
            "\tArithmetic Test   :  2.358307581634193e-05\n",
            "\tRelax state       :  1.1373670739332619e-05\n",
            "for channel  F4\n",
            "\tMirror Image Test :  1.8591146718272506e-05\n",
            "\tStroop Color Test :  1.786607201798924e-05\n",
            "\tArithmetic Test   :  1.7909278085325388e-05\n",
            "\tRelax state       :  1.650308811556642e-05\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}